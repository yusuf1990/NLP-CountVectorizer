{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer with usage example"
      ],
      "metadata": {
        "id": "keFS_0eCN0DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cat_in_the_hat_docs=[\n",
        "      \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n",
        "      \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n",
        "      \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n",
        "      \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n",
        "      \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\"\n",
        "     ]\n",
        "\n",
        "cv = CountVectorizer()\n",
        "count_vector=cv.fit_transform(cat_in_the_hat_docs)"
      ],
      "metadata": {
        "id": "osQpW_GFN0Yg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJqZvlJUPnvL",
        "outputId": "26701955-5b6f-4560-9280-dd79a484ceeb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'one': 28,\n",
              " 'cent': 8,\n",
              " 'two': 40,\n",
              " 'cents': 9,\n",
              " 'old': 26,\n",
              " 'new': 23,\n",
              " 'all': 1,\n",
              " 'about': 0,\n",
              " 'money': 22,\n",
              " 'cat': 7,\n",
              " 'in': 16,\n",
              " 'the': 37,\n",
              " 'hat': 13,\n",
              " 'learning': 19,\n",
              " 'library': 20,\n",
              " 'inside': 18,\n",
              " 'your': 42,\n",
              " 'outside': 30,\n",
              " 'human': 15,\n",
              " 'body': 4,\n",
              " 'oh': 25,\n",
              " 'things': 39,\n",
              " 'you': 41,\n",
              " 'can': 6,\n",
              " 'do': 10,\n",
              " 'that': 36,\n",
              " 'are': 2,\n",
              " 'good': 12,\n",
              " 'for': 11,\n",
              " 'staying': 34,\n",
              " 'healthy': 14,\n",
              " 'on': 27,\n",
              " 'beyond': 3,\n",
              " 'bugs': 5,\n",
              " 'insects': 17,\n",
              " 'there': 38,\n",
              " 'no': 24,\n",
              " 'place': 31,\n",
              " 'like': 21,\n",
              " 'space': 33,\n",
              " 'our': 29,\n",
              " 'solar': 32,\n",
              " 'system': 35}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer With Custom StopWords"
      ],
      "metadata": {
        "id": "pBS5o8YkP7MZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words=[\"all\",\"in\",\"the\",\"is\",\"and\"])\n",
        "count_vector=cv.fit_transform(cat_in_the_hat_docs)\n",
        "count_vector.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfUbVd1yP7vH",
        "outputId": "65cac5d0-9ca1-441f-bf2e-7f1d8ca0f322"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer With Predefined StopWords"
      ],
      "metadata": {
        "id": "11fq0ONVQMqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words=\"english\")\n",
        "count_vector=cv.fit_transform(cat_in_the_hat_docs)"
      ],
      "metadata": {
        "id": "H48ci_Q1QO1C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer with min_df as stopwword"
      ],
      "metadata": {
        "id": "R5RuJy05M-oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text documents\n",
        "documents = [\n",
        "    \"This is a sample document.\",\n",
        "    \"Stop words are common in text processing.\",\n",
        "    \"CountVectorizer is a useful tool.\",\n",
        "    \"This document is just a sample.\",\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer with min_df=2\n",
        "vectorizer = CountVectorizer(min_df=2)\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the vocabulary\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the transformed matrix and vocabulary\n",
        "print(X.toarray())\n",
        "print(vocabulary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v504ueXpM_-c",
        "outputId": "1aa43be2-e110-473c-eb17-0ec9de28f187"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 1]\n",
            " [0 0 0 0]\n",
            " [0 1 0 0]\n",
            " [1 1 1 1]]\n",
            "['document' 'is' 'sample' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with Ngrams"
      ],
      "metadata": {
        "id": "stmG-ZXLMyuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Download the Punkt tokenizer if you haven't already\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtGjhxcnMvd6",
        "outputId": "70b46d73-3f7f-432a-f958-cbe7e8403523"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This is a sample sentence for generating bigrams.\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Generate bigrams (2-grams)\n",
        "bigrams = list(ngrams(words, 2))\n",
        "print(bigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4hvdQi7MBHt",
        "outputId": "a859ff30-4845-4dd1-ad77-522fc1a38266"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'sentence'), ('sentence', 'for'), ('for', 'generating'), ('generating', 'bigrams'), ('bigrams', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text documents\n",
        "documents = [\n",
        "    \"This is a sample document.\",\n",
        "    \"N-grams are useful for text analysis.\",\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer with ngram_range=(1, 2) for unigrams and bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (n-grams)\n",
        "ngrams = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the transformed matrix and n-grams\n",
        "print(X.toarray())\n",
        "print(ngrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF-Xq9tZMIsm",
        "outputId": "3fa48b1b-44cf-4b87-a3a0-9a317e83af51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0]\n",
            " [1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1]]\n",
            "['analysis' 'are' 'are useful' 'document' 'for' 'for text' 'grams'\n",
            " 'grams are' 'is' 'is sample' 'sample' 'sample document' 'text'\n",
            " 'text analysis' 'this' 'this is' 'useful' 'useful for']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Limiting Vocabulary Size"
      ],
      "metadata": {
        "id": "mMyRcvl0xnan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text documents\n",
        "documents = [\n",
        "    \"This is a sample document.\",\n",
        "    \"Limiting vocabulary size is important for NLP tasks.\",\n",
        "    \"You can achieve this by specifying max_features in CountVectorizer.\",\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer with a maximum vocabulary size\n",
        "max_vocab_size = 20  # Set your desired maximum vocabulary size\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=max_vocab_size)\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the transformed matrix and vocabulary\n",
        "print(X.toarray())\n",
        "print(vocabulary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KszFnJHtMcL1",
        "outputId": "d2062017-fac9-4da8-ada3-6020b18698f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0]\n",
            " [1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1]]\n",
            "['achieve' 'by' 'can' 'countvectorizer' 'document' 'for' 'important' 'in'\n",
            " 'is' 'limiting' 'max_features' 'nlp' 'sample' 'size' 'specifying' 'tasks'\n",
            " 'this' 'vocabulary' 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Counts of Words / N-Grams"
      ],
      "metadata": {
        "id": "lpVu5Z5Yy2AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"return n-gram counts in descending order of counts\"\"\"\n",
        "\n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    results=[]\n",
        "\n",
        "    # word index, count i\n",
        "    for idx, count in sorted_items:\n",
        "\n",
        "        # get the ngram name\n",
        "        n_gram=feature_names[idx]\n",
        "\n",
        "        # collect as a list of tuples\n",
        "        results.append((n_gram,count))\n",
        "\n",
        "    return results\n",
        "cv = CountVectorizer(ngram_range=(1,2),max_features=100)\n",
        "count_vector=cv.fit_transform(cat_in_the_hat_docs)\n",
        "\n",
        "#sort the counts of first book title by descending order of counts\n",
        "sorted_items=sort_coo(count_vector[0].tocoo())\n",
        "\n",
        "#Get feature names (words/n-grams). It is sorted by position in sparse matrix\n",
        "feature_names=cv.get_feature_names_out()\n",
        "n_grams=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "n_grams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIckG4Owyz5F",
        "outputId": "508a0de7-1f55-4363-cb35-d69f1e3485ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cent', 3),\n",
              " ('two cents', 1),\n",
              " ('two', 1),\n",
              " ('the hat', 1),\n",
              " ('the', 1),\n",
              " ('one cent', 1),\n",
              " ('one', 1),\n",
              " ('old cent', 1),\n",
              " ('old', 1),\n",
              " ('new cent', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Tokenizer"
      ],
      "metadata": {
        "id": "ihwKZMnWz0cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def my_tokenizer(text):\n",
        "    text=re.sub(\"(\\\\W)\",\" \\\\1 \",text)\n",
        "    return re.split(\"\\\\s+\",text)\n",
        "\n",
        "\n",
        "cv = CountVectorizer(tokenizer=my_tokenizer)\n",
        "count_vector=cv.fit_transform(cat_in_the_hat_docs)\n",
        "print(cv.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyFnntBkxtTY",
        "outputId": "b1c58ba4-cd61-4749-d24a-81e45258055f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'one': 34, 'cent': 14, ',': 4, 'two': 47, 'cents': 15, 'old': 32, 'new': 29, ':': 5, 'all': 7, 'about': 6, 'money': 28, '(': 2, 'cat': 13, 'in': 22, 'the': 44, 'hat': 19, \"'\": 1, 's': 38, 'learning': 25, 'library': 26, 'inside': 24, 'your': 49, 'outside': 36, 'human': 21, 'body': 10, ')': 3, '': 0, 'oh': 31, 'things': 46, 'you': 48, 'can': 12, 'do': 16, 'that': 43, 'are': 8, 'good': 18, 'for': 17, 'staying': 41, 'healthy': 20, 'on': 33, 'beyond': 9, 'bugs': 11, 'insects': 23, 'there': 45, 'no': 30, 'place': 37, 'like': 27, 'space': 40, 'our': 35, 'solar': 39, 'system': 42}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnyjqGIh0j8T"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}